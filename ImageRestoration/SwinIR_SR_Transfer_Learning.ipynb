{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SwinIR-SR-Transfer-Learning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMUkdLf8slEGvKlYNR2qh3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditiAgrawal95/CAECompressionSWinIRRestoration/blob/main/ImageRestoration/SwinIR_SR_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SwinIR code from https://github.com/JingyunLiang/SwinIR and change to SwinIR directory\n",
        "%cd SwinIR/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYNorKsqTmsy",
        "outputId": "92a2e524-cb3a-422c-9604-d9b414316e5d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'SwinIR/'\n",
            "/Users/arushisingh/SwinIR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models.network_swinir import SwinIR as net"
      ],
      "metadata": {
        "id": "wHiRmSoPTvKt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Util to convert to different formats { form-width: \"20%\", display-mode: \"form\" }\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from torchvision.utils import make_grid\n",
        "from datetime import datetime\n",
        "# import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# Kai Zhang (github: https://github.com/cszn)\n",
        "# 03/Mar/2019\n",
        "# --------------------------------------------\n",
        "# https://github.com/twhui/SRGAN-pyTorch\n",
        "# https://github.com/xinntao/BasicSR\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tif']\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "\n",
        "\n",
        "def imshow(x, title=None, cbar=False, figsize=None):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(np.squeeze(x), interpolation='nearest', cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def surf(Z, cmap='rainbow', figsize=None):\n",
        "    plt.figure(figsize=figsize)\n",
        "    ax3 = plt.axes(projection='3d')\n",
        "\n",
        "    w, h = Z.shape[:2]\n",
        "    xx = np.arange(0,w,1)\n",
        "    yy = np.arange(0,h,1)\n",
        "    X, Y = np.meshgrid(xx, yy)\n",
        "    ax3.plot_surface(X,Y,Z,cmap=cmap)\n",
        "    #ax3.contour(X,Y,Z, zdim='z',offset=-2ï¼Œcmap=cmap)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# get image pathes\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def get_image_paths(dataroot):\n",
        "    paths = None  # return None if dataroot is None\n",
        "    if isinstance(dataroot, str):\n",
        "        paths = sorted(_get_paths_from_images(dataroot))\n",
        "    elif isinstance(dataroot, list):\n",
        "        paths = []\n",
        "        for i in dataroot:\n",
        "            paths += sorted(_get_paths_from_images(i))\n",
        "    return paths\n",
        "\n",
        "\n",
        "def _get_paths_from_images(path):\n",
        "    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n",
        "    images = []\n",
        "    for dirpath, _, fnames in sorted(os.walk(path)):\n",
        "        for fname in sorted(fnames):\n",
        "            if is_image_file(fname):\n",
        "                img_path = os.path.join(dirpath, fname)\n",
        "                images.append(img_path)\n",
        "    assert images, '{:s} has no valid image file'.format(path)\n",
        "    return images\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# split large images into small images \n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def patches_from_image(img, p_size=512, p_overlap=64, p_max=800):\n",
        "    w, h = img.shape[:2]\n",
        "    patches = []\n",
        "    if w > p_max and h > p_max:\n",
        "        w1 = list(np.arange(0, w-p_size, p_size-p_overlap, dtype=np.int))\n",
        "        h1 = list(np.arange(0, h-p_size, p_size-p_overlap, dtype=np.int))\n",
        "        w1.append(w-p_size)\n",
        "        h1.append(h-p_size)\n",
        "        # print(w1)\n",
        "        # print(h1)\n",
        "        for i in w1:\n",
        "            for j in h1:\n",
        "                patches.append(img[i:i+p_size, j:j+p_size,:])\n",
        "    else:\n",
        "        patches.append(img)\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n",
        "def imssave(imgs, img_path):\n",
        "    \"\"\"\n",
        "    imgs: list, N images of size WxHxC\n",
        "    \"\"\"\n",
        "    img_name, ext = os.path.splitext(os.path.basename(img_path))\n",
        "    for i, img in enumerate(imgs):\n",
        "        if img.ndim == 3:\n",
        "            img = img[:, :, [2, 1, 0]]\n",
        "        new_path = os.path.join(os.path.dirname(img_path), img_name+str('_{:04d}'.format(i))+'.png')\n",
        "        cv2.imwrite(new_path, img)\n",
        "\n",
        "\n",
        "def split_imageset(original_dataroot, taget_dataroot, n_channels=3, p_size=512, p_overlap=96, p_max=800):\n",
        "    \"\"\"\n",
        "    split the large images from original_dataroot into small overlapped images with size (p_size)x(p_size), \n",
        "    and save them into taget_dataroot; only the images with larger size than (p_max)x(p_max)\n",
        "    will be splitted.\n",
        "\n",
        "    Args:\n",
        "        original_dataroot:\n",
        "        taget_dataroot:\n",
        "        p_size: size of small images\n",
        "        p_overlap: patch size in training is a good choice\n",
        "        p_max: images with smaller size than (p_max)x(p_max) keep unchanged.\n",
        "    \"\"\"\n",
        "    paths = get_image_paths(original_dataroot)\n",
        "    for img_path in paths:\n",
        "        # img_name, ext = os.path.splitext(os.path.basename(img_path))\n",
        "        img = imread_uint(img_path, n_channels=n_channels)\n",
        "        patches = patches_from_image(img, p_size, p_overlap, p_max)\n",
        "        imssave(patches, os.path.join(taget_dataroot, os.path.basename(img_path)))\n",
        "        #if original_dataroot == taget_dataroot:\n",
        "        #del img_path\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# makedir\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, str):\n",
        "        mkdir(paths)\n",
        "    else:\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "\n",
        "\n",
        "def mkdir_and_rename(path):\n",
        "    if os.path.exists(path):\n",
        "        new_name = path + '_archived_' + get_timestamp()\n",
        "        print('Path already exists. Rename it to [{:s}]'.format(new_name))\n",
        "        os.rename(path, new_name)\n",
        "    os.makedirs(path)\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# read image from path\n",
        "# opencv is fast, but read BGR numpy image\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# get uint8 image of size HxWxn_channles (RGB)\n",
        "# --------------------------------------------\n",
        "def imread_uint(path, n_channels=3):\n",
        "    #  input: path\n",
        "    # output: HxWx3(RGB or GGG), or HxWx1 (G)\n",
        "    if n_channels == 1:\n",
        "        img = cv2.imread(path, 0)  # cv2.IMREAD_GRAYSCALE\n",
        "        img = np.expand_dims(img, axis=2)  # HxWx1\n",
        "    elif n_channels == 3:\n",
        "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # BGR or G\n",
        "        if img.ndim == 2:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # GGG\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n",
        "    return img\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# matlab's imwrite\n",
        "# --------------------------------------------\n",
        "def imsave(img, img_path):\n",
        "    img = np.squeeze(img)\n",
        "    if img.ndim == 3:\n",
        "        img = img[:, :, [2, 1, 0]]\n",
        "    cv2.imwrite(img_path, img)\n",
        "\n",
        "def imwrite(img, img_path):\n",
        "    img = np.squeeze(img)\n",
        "    if img.ndim == 3:\n",
        "        img = img[:, :, [2, 1, 0]]\n",
        "    cv2.imwrite(img_path, img)\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# get single image of size HxWxn_channles (BGR)\n",
        "# --------------------------------------------\n",
        "def read_img(path):\n",
        "    # read image by cv2\n",
        "    # return: Numpy float32, HWC, BGR, [0,1]\n",
        "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_GRAYSCALE\n",
        "    img = img.astype(np.float32) / 255.\n",
        "    if img.ndim == 2:\n",
        "        img = np.expand_dims(img, axis=2)\n",
        "    # some images have 4 channels\n",
        "    if img.shape[2] > 3:\n",
        "        img = img[:, :, :3]\n",
        "    return img\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# image format conversion\n",
        "# --------------------------------------------\n",
        "# numpy(single) <--->  numpy(uint)\n",
        "# numpy(single) <--->  tensor\n",
        "# numpy(uint)   <--->  tensor\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# numpy(single) [0, 1] <--->  numpy(uint)\n",
        "# --------------------------------------------\n",
        "\n",
        "\n",
        "def uint2single(img):\n",
        "\n",
        "    return np.float32(img/255.)\n",
        "\n",
        "\n",
        "def single2uint(img):\n",
        "\n",
        "    return np.uint8((img.clip(0, 1)*255.).round())\n",
        "\n",
        "\n",
        "def uint162single(img):\n",
        "\n",
        "    return np.float32(img/65535.)\n",
        "\n",
        "\n",
        "def single2uint16(img):\n",
        "\n",
        "    return np.uint16((img.clip(0, 1)*65535.).round())\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# numpy(uint) (HxWxC or HxW) <--->  tensor\n",
        "# --------------------------------------------\n",
        "\n",
        "\n",
        "# convert uint to 4-dimensional torch tensor\n",
        "def uint2tensor4(img):\n",
        "    if img.ndim == 2:\n",
        "        img = np.expand_dims(img, axis=2)\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.).unsqueeze(0)\n",
        "\n",
        "\n",
        "# convert uint to 3-dimensional torch tensor\n",
        "def uint2tensor3(img):\n",
        "    if img.ndim == 2:\n",
        "        img = np.expand_dims(img, axis=2)\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.)\n",
        "\n",
        "\n",
        "# convert 2/3/4-dimensional torch tensor to uint\n",
        "def tensor2uint(img):\n",
        "    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()\n",
        "    if img.ndim == 3:\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "    return np.uint8((img*255.0).round())\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# numpy(single) (HxWxC) <--->  tensor\n",
        "# --------------------------------------------\n",
        "\n",
        "\n",
        "# convert single (HxWxC) to 3-dimensional torch tensor\n",
        "def single2tensor3(img):\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float()\n",
        "\n",
        "\n",
        "# convert single (HxWxC) to 4-dimensional torch tensor\n",
        "def single2tensor4(img):\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().unsqueeze(0)\n",
        "\n",
        "\n",
        "# convert torch tensor to single\n",
        "def tensor2single(img):\n",
        "    img = img.data.squeeze().float().cpu().numpy()\n",
        "    if img.ndim == 3:\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "    return img\n",
        "\n",
        "# convert torch tensor to single\n",
        "def tensor2single3(img):\n",
        "    img = img.data.squeeze().float().cpu().numpy()\n",
        "    if img.ndim == 3:\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "    elif img.ndim == 2:\n",
        "        img = np.expand_dims(img, axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def single2tensor5(img):\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float().unsqueeze(0)\n",
        "\n",
        "\n",
        "def single32tensor5(img):\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def single42tensor4(img):\n",
        "    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float()\n",
        "\n",
        "\n",
        "# from skimage.io import imread, imsave\n",
        "def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
        "    '''\n",
        "    Converts a torch Tensor into an image Numpy array of BGR channel order\n",
        "    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n",
        "    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n",
        "    '''\n",
        "    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # squeeze first, then clamp\n",
        "    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n",
        "    n_dim = tensor.dim()\n",
        "    if n_dim == 4:\n",
        "        n_img = len(tensor)\n",
        "        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n",
        "        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n",
        "    elif n_dim == 3:\n",
        "        img_np = tensor.numpy()\n",
        "        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n",
        "    elif n_dim == 2:\n",
        "        img_np = tensor.numpy()\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}'.format(n_dim))\n",
        "    if out_type == np.uint8:\n",
        "        img_np = (img_np * 255.0).round()\n",
        "        # Important. Unlike matlab, numpy.uint8() WILL NOT round by default.\n",
        "    return img_np.astype(out_type)\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# Augmentation, flipe and/or rotate\n",
        "# --------------------------------------------\n",
        "# The following two are enough.\n",
        "# (1) augmet_img: numpy image of WxHxC or WxH\n",
        "# (2) augment_img_tensor4: tensor image 1xCxWxH\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def augment_img(img, mode=0):\n",
        "    '''Kai Zhang (github: https://github.com/cszn)\n",
        "    '''\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 2:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 3:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 4:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 5:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))\n",
        "\n",
        "\n",
        "def augment_img_tensor4(img, mode=0):\n",
        "    '''Kai Zhang (github: https://github.com/cszn)\n",
        "    '''\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return img.rot90(1, [2, 3]).flip([2])\n",
        "    elif mode == 2:\n",
        "        return img.flip([2])\n",
        "    elif mode == 3:\n",
        "        return img.rot90(3, [2, 3])\n",
        "    elif mode == 4:\n",
        "        return img.rot90(2, [2, 3]).flip([2])\n",
        "    elif mode == 5:\n",
        "        return img.rot90(1, [2, 3])\n",
        "    elif mode == 6:\n",
        "        return img.rot90(2, [2, 3])\n",
        "    elif mode == 7:\n",
        "        return img.rot90(3, [2, 3]).flip([2])\n",
        "\n",
        "\n",
        "def augment_img_tensor(img, mode=0):\n",
        "    '''Kai Zhang (github: https://github.com/cszn)\n",
        "    '''\n",
        "    img_size = img.size()\n",
        "    img_np = img.data.cpu().numpy()\n",
        "    if len(img_size) == 3:\n",
        "        img_np = np.transpose(img_np, (1, 2, 0))\n",
        "    elif len(img_size) == 4:\n",
        "        img_np = np.transpose(img_np, (2, 3, 1, 0))\n",
        "    img_np = augment_img(img_np, mode=mode)\n",
        "    img_tensor = torch.from_numpy(np.ascontiguousarray(img_np))\n",
        "    if len(img_size) == 3:\n",
        "        img_tensor = img_tensor.permute(2, 0, 1)\n",
        "    elif len(img_size) == 4:\n",
        "        img_tensor = img_tensor.permute(3, 2, 0, 1)\n",
        "\n",
        "    return img_tensor.type_as(img)\n",
        "\n",
        "\n",
        "def augment_img_np3(img, mode=0):\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return img.transpose(1, 0, 2)\n",
        "    elif mode == 2:\n",
        "        return img[::-1, :, :]\n",
        "    elif mode == 3:\n",
        "        img = img[::-1, :, :]\n",
        "        img = img.transpose(1, 0, 2)\n",
        "        return img\n",
        "    elif mode == 4:\n",
        "        return img[:, ::-1, :]\n",
        "    elif mode == 5:\n",
        "        img = img[:, ::-1, :]\n",
        "        img = img.transpose(1, 0, 2)\n",
        "        return img\n",
        "    elif mode == 6:\n",
        "        img = img[:, ::-1, :]\n",
        "        img = img[::-1, :, :]\n",
        "        return img\n",
        "    elif mode == 7:\n",
        "        img = img[:, ::-1, :]\n",
        "        img = img[::-1, :, :]\n",
        "        img = img.transpose(1, 0, 2)\n",
        "        return img\n",
        "\n",
        "\n",
        "def augment_imgs(img_list, hflip=True, rot=True):\n",
        "    # horizontal flip OR rotate\n",
        "    hflip = hflip and random.random() < 0.5\n",
        "    vflip = rot and random.random() < 0.5\n",
        "    rot90 = rot and random.random() < 0.5\n",
        "\n",
        "    def _augment(img):\n",
        "        if hflip:\n",
        "            img = img[:, ::-1, :]\n",
        "        if vflip:\n",
        "            img = img[::-1, :, :]\n",
        "        if rot90:\n",
        "            img = img.transpose(1, 0, 2)\n",
        "        return img\n",
        "\n",
        "    return [_augment(img) for img in img_list]\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# modcrop and shave\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def modcrop(img_in, scale):\n",
        "    # img_in: Numpy, HWC or HW\n",
        "    img = np.copy(img_in)\n",
        "    if img.ndim == 2:\n",
        "        H, W = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r]\n",
        "    elif img.ndim == 3:\n",
        "        H, W, C = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r, :]\n",
        "    else:\n",
        "        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n",
        "    return img\n",
        "\n",
        "\n",
        "def shave(img_in, border=0):\n",
        "    # img_in: Numpy, HWC or HW\n",
        "    img = np.copy(img_in)\n",
        "    h, w = img.shape[:2]\n",
        "    img = img[border:h-border, border:w-border]\n",
        "    return img\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# image processing process on numpy image\n",
        "# channel_convert(in_c, tar_type, img_list):\n",
        "# rgb2ycbcr(img, only_y=True):\n",
        "# bgr2ycbcr(img, only_y=True):\n",
        "# ycbcr2rgb(img):\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "def rgb2ycbcr(img, only_y=True):\n",
        "    '''same as matlab rgb2ycbcr\n",
        "    only_y: only return Y channel\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img *= 255.\n",
        "    # convert\n",
        "    if only_y:\n",
        "        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n",
        "    else:\n",
        "        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n",
        "                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "\n",
        "\n",
        "def ycbcr2rgb(img):\n",
        "    '''same as matlab ycbcr2rgb\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img *= 255.\n",
        "    # convert\n",
        "    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n",
        "                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n",
        "    rlt = np.clip(rlt, 0, 255)\n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "\n",
        "\n",
        "def bgr2ycbcr(img, only_y=True):\n",
        "    '''bgr version of rgb2ycbcr\n",
        "    only_y: only return Y channel\n",
        "    Input:\n",
        "        uint8, [0, 255]\n",
        "        float, [0, 1]\n",
        "    '''\n",
        "    in_img_type = img.dtype\n",
        "    img.astype(np.float32)\n",
        "    if in_img_type != np.uint8:\n",
        "        img *= 255.\n",
        "    # convert\n",
        "    if only_y:\n",
        "        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n",
        "    else:\n",
        "        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n",
        "                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n",
        "    if in_img_type == np.uint8:\n",
        "        rlt = rlt.round()\n",
        "    else:\n",
        "        rlt /= 255.\n",
        "    return rlt.astype(in_img_type)\n",
        "\n",
        "\n",
        "def channel_convert(in_c, tar_type, img_list):\n",
        "    # conversion among BGR, gray and y\n",
        "    if in_c == 3 and tar_type == 'gray':  # BGR to gray\n",
        "        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n",
        "        return [np.expand_dims(img, axis=2) for img in gray_list]\n",
        "    elif in_c == 3 and tar_type == 'y':  # BGR to y\n",
        "        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n",
        "        return [np.expand_dims(img, axis=2) for img in y_list]\n",
        "    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR\n",
        "        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n",
        "    else:\n",
        "        return img_list\n",
        "\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# metric, PSNR, SSIM and PSNRB\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# PSNR\n",
        "# --------------------------------------------\n",
        "def calculate_psnr(img1, img2, border=0):\n",
        "    # img1 and img2 have range [0, 255]\n",
        "    #img1 = img1.squeeze()\n",
        "    #img2 = img2.squeeze()\n",
        "    if not img1.shape == img2.shape:\n",
        "        raise ValueError('Input images must have the same dimensions.')\n",
        "    h, w = img1.shape[:2]\n",
        "    img1 = img1[border:h-border, border:w-border]\n",
        "    img2 = img2[border:h-border, border:w-border]\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    mse = np.mean((img1 - img2)**2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    return 20 * math.log10(255.0 / math.sqrt(mse))\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# SSIM\n",
        "# --------------------------------------------\n",
        "def calculate_ssim(img1, img2, border=0):\n",
        "    '''calculate SSIM\n",
        "    the same outputs as MATLAB's\n",
        "    img1, img2: [0, 255]\n",
        "    '''\n",
        "    #img1 = img1.squeeze()\n",
        "    #img2 = img2.squeeze()\n",
        "    if not img1.shape == img2.shape:\n",
        "        raise ValueError('Input images must have the same dimensions.')\n",
        "    h, w = img1.shape[:2]\n",
        "    img1 = img1[border:h-border, border:w-border]\n",
        "    img2 = img2[border:h-border, border:w-border]\n",
        "\n",
        "    if img1.ndim == 2:\n",
        "        return ssim(img1, img2)\n",
        "    elif img1.ndim == 3:\n",
        "        if img1.shape[2] == 3:\n",
        "            ssims = []\n",
        "            for i in range(3):\n",
        "                ssims.append(ssim(img1[:,:,i], img2[:,:,i]))\n",
        "            return np.array(ssims).mean()\n",
        "        elif img1.shape[2] == 1:\n",
        "            return ssim(np.squeeze(img1), np.squeeze(img2))\n",
        "    else:\n",
        "        raise ValueError('Wrong input image dimensions.')\n",
        "\n",
        "\n",
        "def ssim(img1, img2):\n",
        "    C1 = (0.01 * 255)**2\n",
        "    C2 = (0.03 * 255)**2\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
        "    window = np.outer(kernel, kernel.transpose())\n",
        "\n",
        "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n",
        "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
        "    mu1_sq = mu1**2\n",
        "    mu2_sq = mu2**2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
        "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
        "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
        "                                                            (sigma1_sq + sigma2_sq + C2))\n",
        "    return ssim_map.mean()\n",
        "\n",
        "\n",
        "def _blocking_effect_factor(im):\n",
        "    block_size = 8\n",
        "\n",
        "    block_horizontal_positions = torch.arange(7, im.shape[3] - 1, 8)\n",
        "    block_vertical_positions = torch.arange(7, im.shape[2] - 1, 8)\n",
        "\n",
        "    horizontal_block_difference = (\n",
        "                (im[:, :, :, block_horizontal_positions] - im[:, :, :, block_horizontal_positions + 1]) ** 2).sum(\n",
        "        3).sum(2).sum(1)\n",
        "    vertical_block_difference = (\n",
        "                (im[:, :, block_vertical_positions, :] - im[:, :, block_vertical_positions + 1, :]) ** 2).sum(3).sum(\n",
        "        2).sum(1)\n",
        "\n",
        "    nonblock_horizontal_positions = np.setdiff1d(torch.arange(0, im.shape[3] - 1), block_horizontal_positions)\n",
        "    nonblock_vertical_positions = np.setdiff1d(torch.arange(0, im.shape[2] - 1), block_vertical_positions)\n",
        "\n",
        "    horizontal_nonblock_difference = (\n",
        "                (im[:, :, :, nonblock_horizontal_positions] - im[:, :, :, nonblock_horizontal_positions + 1]) ** 2).sum(\n",
        "        3).sum(2).sum(1)\n",
        "    vertical_nonblock_difference = (\n",
        "                (im[:, :, nonblock_vertical_positions, :] - im[:, :, nonblock_vertical_positions + 1, :]) ** 2).sum(\n",
        "        3).sum(2).sum(1)\n",
        "\n",
        "    n_boundary_horiz = im.shape[2] * (im.shape[3] // block_size - 1)\n",
        "    n_boundary_vert = im.shape[3] * (im.shape[2] // block_size - 1)\n",
        "    boundary_difference = (horizontal_block_difference + vertical_block_difference) / (\n",
        "                n_boundary_horiz + n_boundary_vert)\n",
        "\n",
        "    n_nonboundary_horiz = im.shape[2] * (im.shape[3] - 1) - n_boundary_horiz\n",
        "    n_nonboundary_vert = im.shape[3] * (im.shape[2] - 1) - n_boundary_vert\n",
        "    nonboundary_difference = (horizontal_nonblock_difference + vertical_nonblock_difference) / (\n",
        "                n_nonboundary_horiz + n_nonboundary_vert)\n",
        "\n",
        "    scaler = np.log2(block_size) / np.log2(min([im.shape[2], im.shape[3]]))\n",
        "    bef = scaler * (boundary_difference - nonboundary_difference)\n",
        "\n",
        "    bef[boundary_difference <= nonboundary_difference] = 0\n",
        "    return bef\n",
        "\n",
        "\n",
        "def calculate_psnrb(img1, img2, border=0):\n",
        "    \"\"\"Calculate PSNR-B (Peak Signal-to-Noise Ratio).\n",
        "    Ref: Quality assessment of deblocked images, for JPEG image deblocking evaluation\n",
        "    # https://gitlab.com/Queuecumber/quantization-guided-ac/-/blob/master/metrics/psnrb.py\n",
        "    Args:\n",
        "        img1 (ndarray): Images with range [0, 255].\n",
        "        img2 (ndarray): Images with range [0, 255].\n",
        "        border (int): Cropped pixels in each edge of an image. These\n",
        "            pixels are not involved in the PSNR calculation.\n",
        "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
        "    Returns:\n",
        "        float: psnr result.\n",
        "    \"\"\"\n",
        "\n",
        "    if not img1.shape == img2.shape:\n",
        "        raise ValueError('Input images must have the same dimensions.')\n",
        "\n",
        "    if img1.ndim == 2:\n",
        "        img1, img2 = np.expand_dims(img1, 2), np.expand_dims(img2, 2)\n",
        "\n",
        "    h, w = img1.shape[:2]\n",
        "    img1 = img1[border:h-border, border:w-border]\n",
        "    img2 = img2[border:h-border, border:w-border]\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "\n",
        "    # follow https://gitlab.com/Queuecumber/quantization-guided-ac/-/blob/master/metrics/psnrb.py\n",
        "    img1 = torch.from_numpy(img1).permute(2, 0, 1).unsqueeze(0) / 255.\n",
        "    img2 = torch.from_numpy(img2).permute(2, 0, 1).unsqueeze(0) / 255.\n",
        "\n",
        "    total = 0\n",
        "    for c in range(img1.shape[1]):\n",
        "        mse = torch.nn.functional.mse_loss(img1[:, c:c + 1, :, :], img2[:, c:c + 1, :, :], reduction='none')\n",
        "        bef = _blocking_effect_factor(img1[:, c:c + 1, :, :])\n",
        "\n",
        "        mse = mse.view(mse.shape[0], -1).mean(1)\n",
        "        total += 10 * torch.log10(1 / (mse + bef))\n",
        "\n",
        "    return float(total) / img1.shape[1]\n",
        "\n",
        "'''\n",
        "# --------------------------------------------\n",
        "# matlab's bicubic imresize (numpy and torch) [0, 1]\n",
        "# --------------------------------------------\n",
        "'''\n",
        "\n",
        "\n",
        "# matlab 'imresize' function, now only support 'bicubic'\n",
        "def cubic(x):\n",
        "    absx = torch.abs(x)\n",
        "    absx2 = absx**2\n",
        "    absx3 = absx**3\n",
        "    return (1.5*absx3 - 2.5*absx2 + 1) * ((absx <= 1).type_as(absx)) + \\\n",
        "        (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * (((absx > 1)*(absx <= 2)).type_as(absx))\n",
        "\n",
        "\n",
        "def calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n",
        "    if (scale < 1) and (antialiasing):\n",
        "        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n",
        "        kernel_width = kernel_width / scale\n",
        "\n",
        "    # Output-space coordinates\n",
        "    x = torch.linspace(1, out_length, out_length)\n",
        "\n",
        "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
        "    # in output space maps to 0.5 in input space, and 0.5+scale in output\n",
        "    # space maps to 1.5 in input space.\n",
        "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
        "\n",
        "    # What is the left-most pixel that can be involved in the computation?\n",
        "    left = torch.floor(u - kernel_width / 2)\n",
        "\n",
        "    # What is the maximum number of pixels that can be involved in the\n",
        "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
        "    # corresponding weights are all zero, it will be eliminated at the end\n",
        "    # of this function.\n",
        "    P = math.ceil(kernel_width) + 2\n",
        "\n",
        "    # The indices of the input pixels involved in computing the k-th output\n",
        "    # pixel are in row k of the indices matrix.\n",
        "    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n",
        "        1, P).expand(out_length, P)\n",
        "\n",
        "    # The weights used to compute the k-th output pixel are in row k of the\n",
        "    # weights matrix.\n",
        "    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n",
        "    # apply cubic kernel\n",
        "    if (scale < 1) and (antialiasing):\n",
        "        weights = scale * cubic(distance_to_center * scale)\n",
        "    else:\n",
        "        weights = cubic(distance_to_center)\n",
        "    # Normalize the weights matrix so that each row sums to 1.\n",
        "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
        "    weights = weights / weights_sum.expand(out_length, P)\n",
        "\n",
        "    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n",
        "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
        "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 1, P - 2)\n",
        "        weights = weights.narrow(1, 1, P - 2)\n",
        "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 0, P - 2)\n",
        "        weights = weights.narrow(1, 0, P - 2)\n",
        "    weights = weights.contiguous()\n",
        "    indices = indices.contiguous()\n",
        "    sym_len_s = -indices.min() + 1\n",
        "    sym_len_e = indices.max() - in_length\n",
        "    indices = indices + sym_len_s - 1\n",
        "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# imresize for tensor image [0, 1]\n",
        "# --------------------------------------------\n",
        "def imresize(img, scale, antialiasing=True):\n",
        "    # Now the scale should be the same for H and W\n",
        "    # input: img: pytorch tensor, CHW or HW [0,1]\n",
        "    # output: CHW or HW [0,1] w/o round\n",
        "    need_squeeze = True if img.dim() == 2 else False\n",
        "    if need_squeeze:\n",
        "        img.unsqueeze_(0)\n",
        "    in_C, in_H, in_W = img.size()\n",
        "    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n",
        "    kernel_width = 4\n",
        "    kernel = 'cubic'\n",
        "\n",
        "    # Return the desired dimension order for performing the resize.  The\n",
        "    # strategy is to perform the resize first along the dimension with the\n",
        "    # smallest scale factor.\n",
        "    # Now we do not support this.\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n",
        "        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n",
        "    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n",
        "        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n",
        "    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n",
        "\n",
        "    sym_patch = img[:, :sym_len_Hs, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = img[:, -sym_len_He:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n",
        "    kernel_width = weights_H.size(1)\n",
        "    for i in range(out_H):\n",
        "        idx = int(indices_H[i][0])\n",
        "        for j in range(out_C):\n",
        "            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n",
        "    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :, :sym_len_Ws]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, :, -sym_len_We:]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n",
        "    kernel_width = weights_W.size(1)\n",
        "    for i in range(out_W):\n",
        "        idx = int(indices_W[i][0])\n",
        "        for j in range(out_C):\n",
        "            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_W[i])\n",
        "    if need_squeeze:\n",
        "        out_2.squeeze_()\n",
        "    return out_2\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# imresize for numpy image [0, 1]\n",
        "# --------------------------------------------\n",
        "def imresize_np(img, scale, antialiasing=True):\n",
        "    # Now the scale should be the same for H and W\n",
        "    # input: img: Numpy, HWC or HW [0,1]\n",
        "    # output: HWC or HW [0,1] w/o round\n",
        "    img = torch.from_numpy(img)\n",
        "    need_squeeze = True if img.dim() == 2 else False\n",
        "    if need_squeeze:\n",
        "        img.unsqueeze_(2)\n",
        "\n",
        "    in_H, in_W, in_C = img.size()\n",
        "    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n",
        "    kernel_width = 4\n",
        "    kernel = 'cubic'\n",
        "\n",
        "    # Return the desired dimension order for performing the resize.  The\n",
        "    # strategy is to perform the resize first along the dimension with the\n",
        "    # smallest scale factor.\n",
        "    # Now we do not support this.\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n",
        "        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n",
        "    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n",
        "        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n",
        "    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n",
        "\n",
        "    sym_patch = img[:sym_len_Hs, :, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n",
        "    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = img[-sym_len_He:, :, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n",
        "    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n",
        "    kernel_width = weights_H.size(1)\n",
        "    for i in range(out_H):\n",
        "        idx = int(indices_H[i][0])\n",
        "        for j in range(out_C):\n",
        "            out_1[i, :, j] = img_aug[idx:idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n",
        "    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :sym_len_Ws, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, -sym_len_We:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n",
        "    kernel_width = weights_W.size(1)\n",
        "    for i in range(out_W):\n",
        "        idx = int(indices_W[i][0])\n",
        "        for j in range(out_C):\n",
        "            out_2[:, i, j] = out_1_aug[:, idx:idx + kernel_width, j].mv(weights_W[i])\n",
        "    if need_squeeze:\n",
        "        out_2.squeeze_()\n",
        "\n",
        "    return out_2.numpy()  "
      ],
      "metadata": {
        "id": "WBPvpLlc06hF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate degraded image from high quality image using BSRGAN { form-width: \"20%\", display-mode: \"form\" }\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "import random\n",
        "from scipy import ndimage\n",
        "import scipy\n",
        "import scipy.stats as ss\n",
        "from scipy.interpolate import interp2d\n",
        "from scipy.linalg import orth\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# --------------------------------------------\n",
        "# Super-Resolution\n",
        "# --------------------------------------------\n",
        "#\n",
        "# Kai Zhang (cskaizhang@gmail.com)\n",
        "# https://github.com/cszn\n",
        "# From 2019/03--2021/08\n",
        "# --------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "def modcrop_np(img, sf):\n",
        "    '''\n",
        "    Args:\n",
        "        img: numpy image, WxH or WxHxC\n",
        "        sf: scale factor\n",
        "    Return:\n",
        "        cropped image\n",
        "    '''\n",
        "    w, h = img.shape[:2]\n",
        "    im = np.copy(img)\n",
        "    return im[:w - w % sf, :h - h % sf, ...]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# --------------------------------------------\n",
        "# anisotropic Gaussian kernels\n",
        "# --------------------------------------------\n",
        "\"\"\"\n",
        "def analytic_kernel(k):\n",
        "    \"\"\"Calculate the X4 kernel from the X2 kernel (for proof see appendix in paper)\"\"\"\n",
        "    k_size = k.shape[0]\n",
        "    # Calculate the big kernels size\n",
        "    big_k = np.zeros((3 * k_size - 2, 3 * k_size - 2))\n",
        "    # Loop over the small kernel to fill the big one\n",
        "    for r in range(k_size):\n",
        "        for c in range(k_size):\n",
        "            big_k[2 * r:2 * r + k_size, 2 * c:2 * c + k_size] += k[r, c] * k\n",
        "    # Crop the edges of the big kernel to ignore very small values and increase run time of SR\n",
        "    crop = k_size // 2\n",
        "    cropped_big_k = big_k[crop:-crop, crop:-crop]\n",
        "    # Normalize to 1\n",
        "    return cropped_big_k / cropped_big_k.sum()\n",
        "\n",
        "\n",
        "def anisotropic_Gaussian(ksize=15, theta=np.pi, l1=6, l2=6):\n",
        "    \"\"\" generate an anisotropic Gaussian kernel\n",
        "    Args:\n",
        "        ksize : e.g., 15, kernel size\n",
        "        theta : [0,  pi], rotation angle range\n",
        "        l1    : [0.1,50], scaling of eigenvalues\n",
        "        l2    : [0.1,l1], scaling of eigenvalues\n",
        "        If l1 = l2, will get an isotropic Gaussian kernel.\n",
        "    Returns:\n",
        "        k     : kernel\n",
        "    \"\"\"\n",
        "\n",
        "    v = np.dot(np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]), np.array([1., 0.]))\n",
        "    V = np.array([[v[0], v[1]], [v[1], -v[0]]])\n",
        "    D = np.array([[l1, 0], [0, l2]])\n",
        "    Sigma = np.dot(np.dot(V, D), np.linalg.inv(V))\n",
        "    k = gm_blur_kernel(mean=[0, 0], cov=Sigma, size=ksize)\n",
        "\n",
        "    return k\n",
        "\n",
        "\n",
        "def gm_blur_kernel(mean, cov, size=15):\n",
        "    center = size / 2.0 + 0.5\n",
        "    k = np.zeros([size, size])\n",
        "    for y in range(size):\n",
        "        for x in range(size):\n",
        "            cy = y - center + 1\n",
        "            cx = x - center + 1\n",
        "            k[y, x] = ss.multivariate_normal.pdf([cx, cy], mean=mean, cov=cov)\n",
        "\n",
        "    k = k / np.sum(k)\n",
        "    return k\n",
        "\n",
        "\n",
        "def shift_pixel(x, sf, upper_left=True):\n",
        "    \"\"\"shift pixel for super-resolution with different scale factors\n",
        "    Args:\n",
        "        x: WxHxC or WxH\n",
        "        sf: scale factor\n",
        "        upper_left: shift direction\n",
        "    \"\"\"\n",
        "    h, w = x.shape[:2]\n",
        "    shift = (sf-1)*0.5\n",
        "    xv, yv = np.arange(0, w, 1.0), np.arange(0, h, 1.0)\n",
        "    if upper_left:\n",
        "        x1 = xv + shift\n",
        "        y1 = yv + shift\n",
        "    else:\n",
        "        x1 = xv - shift\n",
        "        y1 = yv - shift\n",
        "\n",
        "    x1 = np.clip(x1, 0, w-1)\n",
        "    y1 = np.clip(y1, 0, h-1)\n",
        "\n",
        "    if x.ndim == 2:\n",
        "        x = interp2d(xv, yv, x)(x1, y1)\n",
        "    if x.ndim == 3:\n",
        "        for i in range(x.shape[-1]):\n",
        "            x[:, :, i] = interp2d(xv, yv, x[:, :, i])(x1, y1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def blur(x, k):\n",
        "    '''\n",
        "    x: image, NxcxHxW\n",
        "    k: kernel, Nx1xhxw\n",
        "    '''\n",
        "    n, c = x.shape[:2]\n",
        "    p1, p2 = (k.shape[-2]-1)//2, (k.shape[-1]-1)//2\n",
        "    x = torch.nn.functional.pad(x, pad=(p1, p2, p1, p2), mode='replicate')\n",
        "    k = k.repeat(1,c,1,1)\n",
        "    k = k.view(-1, 1, k.shape[2], k.shape[3])\n",
        "    x = x.view(1, -1, x.shape[2], x.shape[3])\n",
        "    x = torch.nn.functional.conv2d(x, k, bias=None, stride=1, padding=0, groups=n*c)\n",
        "    x = x.view(n, c, x.shape[2], x.shape[3])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def gen_kernel(k_size=np.array([15, 15]), scale_factor=np.array([4, 4]), min_var=0.6, max_var=10., noise_level=0):\n",
        "    \"\"\"\"\n",
        "    # modified version of https://github.com/assafshocher/BlindSR_dataset_generator\n",
        "    # Kai Zhang\n",
        "    # min_var = 0.175 * sf  # variance of the gaussian kernel will be sampled between min_var and max_var\n",
        "    # max_var = 2.5 * sf\n",
        "    \"\"\"\n",
        "    # Set random eigen-vals (lambdas) and angle (theta) for COV matrix\n",
        "    lambda_1 = min_var + np.random.rand() * (max_var - min_var)\n",
        "    lambda_2 = min_var + np.random.rand() * (max_var - min_var)\n",
        "    theta = np.random.rand() * np.pi  # random theta\n",
        "    noise = -noise_level + np.random.rand(*k_size) * noise_level * 2\n",
        "\n",
        "    # Set COV matrix using Lambdas and Theta\n",
        "    LAMBDA = np.diag([lambda_1, lambda_2])\n",
        "    Q = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                  [np.sin(theta), np.cos(theta)]])\n",
        "    SIGMA = Q @ LAMBDA @ Q.T\n",
        "    INV_SIGMA = np.linalg.inv(SIGMA)[None, None, :, :]\n",
        "\n",
        "    # Set expectation position (shifting kernel for aligned image)\n",
        "    MU = k_size // 2 - 0.5*(scale_factor - 1) # - 0.5 * (scale_factor - k_size % 2)\n",
        "    MU = MU[None, None, :, None]\n",
        "\n",
        "    # Create meshgrid for Gaussian\n",
        "    [X,Y] = np.meshgrid(range(k_size[0]), range(k_size[1]))\n",
        "    Z = np.stack([X, Y], 2)[:, :, :, None]\n",
        "\n",
        "    # Calcualte Gaussian for every pixel of the kernel\n",
        "    ZZ = Z-MU\n",
        "    ZZ_t = ZZ.transpose(0,1,3,2)\n",
        "    raw_kernel = np.exp(-0.5 * np.squeeze(ZZ_t @ INV_SIGMA @ ZZ)) * (1 + noise)\n",
        "\n",
        "    # shift the kernel so it will be centered\n",
        "    #raw_kernel_centered = kernel_shift(raw_kernel, scale_factor)\n",
        "\n",
        "    # Normalize the kernel and return\n",
        "    #kernel = raw_kernel_centered / np.sum(raw_kernel_centered)\n",
        "    kernel = raw_kernel / np.sum(raw_kernel)\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def fspecial_gaussian(hsize, sigma):\n",
        "    hsize = [hsize, hsize]\n",
        "    siz = [(hsize[0]-1.0)/2.0, (hsize[1]-1.0)/2.0]\n",
        "    std = sigma\n",
        "    [x, y] = np.meshgrid(np.arange(-siz[1], siz[1]+1), np.arange(-siz[0], siz[0]+1))\n",
        "    arg = -(x*x + y*y)/(2*std*std)\n",
        "    h = np.exp(arg)\n",
        "    h[h < scipy.finfo(float).eps * h.max()] = 0\n",
        "    sumh = h.sum()\n",
        "    if sumh != 0:\n",
        "        h = h/sumh\n",
        "    return h\n",
        "\n",
        "\n",
        "def fspecial_laplacian(alpha):\n",
        "    alpha = max([0, min([alpha,1])])\n",
        "    h1 = alpha/(alpha+1)\n",
        "    h2 = (1-alpha)/(alpha+1)\n",
        "    h = [[h1, h2, h1], [h2, -4/(alpha+1), h2], [h1, h2, h1]]\n",
        "    h = np.array(h)\n",
        "    return h\n",
        "\n",
        "\n",
        "def fspecial(filter_type, *args, **kwargs):\n",
        "    '''\n",
        "    python code from:\n",
        "    https://github.com/ronaldosena/imagens-medicas-2/blob/40171a6c259edec7827a6693a93955de2bd39e76/Aulas/aula_2_-_uniform_filter/matlab_fspecial.py\n",
        "    '''\n",
        "    if filter_type == 'gaussian':\n",
        "        return fspecial_gaussian(*args, **kwargs)\n",
        "    if filter_type == 'laplacian':\n",
        "        return fspecial_laplacian(*args, **kwargs)\n",
        "\n",
        "\"\"\"\n",
        "# --------------------------------------------\n",
        "# degradation models\n",
        "# --------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def bicubic_degradation(x, sf=3):\n",
        "    '''\n",
        "    Args:\n",
        "        x: HxWxC image, [0, 1]\n",
        "        sf: down-scale factor\n",
        "    Return:\n",
        "        bicubicly downsampled LR image\n",
        "    '''\n",
        "    x = imresize_np(x, scale=1/sf)\n",
        "    return x\n",
        "\n",
        "\n",
        "def srmd_degradation(x, k, sf=3):\n",
        "    ''' blur + bicubic downsampling\n",
        "    Args:\n",
        "        x: HxWxC image, [0, 1]\n",
        "        k: hxw, double\n",
        "        sf: down-scale factor\n",
        "    Return:\n",
        "        downsampled LR image\n",
        "    Reference:\n",
        "        @inproceedings{zhang2018learning,\n",
        "          title={Learning a single convolutional super-resolution network for multiple degradations},\n",
        "          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n",
        "          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "          pages={3262--3271},\n",
        "          year={2018}\n",
        "        }\n",
        "    '''\n",
        "    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')  # 'nearest' | 'mirror'\n",
        "    x = bicubic_degradation(x, sf=sf)\n",
        "    return x\n",
        "\n",
        "\n",
        "def dpsr_degradation(x, k, sf=3):\n",
        "\n",
        "    ''' bicubic downsampling + blur\n",
        "    Args:\n",
        "        x: HxWxC image, [0, 1]\n",
        "        k: hxw, double\n",
        "        sf: down-scale factor\n",
        "    Return:\n",
        "        downsampled LR image\n",
        "    Reference:\n",
        "        @inproceedings{zhang2019deep,\n",
        "          title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n",
        "          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n",
        "          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n",
        "          pages={1671--1681},\n",
        "          year={2019}\n",
        "        }\n",
        "    '''\n",
        "    x = bicubic_degradation(x, sf=sf)\n",
        "    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n",
        "    return x\n",
        "\n",
        "\n",
        "def classical_degradation(x, k, sf=3):\n",
        "    ''' blur + downsampling\n",
        "    Args:\n",
        "        x: HxWxC image, [0, 1]/[0, 255]\n",
        "        k: hxw, double\n",
        "        sf: down-scale factor\n",
        "    Return:\n",
        "        downsampled LR image\n",
        "    '''\n",
        "    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode='wrap')\n",
        "    #x = filters.correlate(x, np.expand_dims(np.flip(k), axis=2))\n",
        "    st = 0\n",
        "    return x[st::sf, st::sf, ...]\n",
        "\n",
        "\n",
        "def add_sharpening(img, weight=0.5, radius=50, threshold=10):\n",
        "    \"\"\"USM sharpening. borrowed from real-ESRGAN\n",
        "    Input image: I; Blurry image: B.\n",
        "    1. K = I + weight * (I - B)\n",
        "    2. Mask = 1 if abs(I - B) > threshold, else: 0\n",
        "    3. Blur mask:\n",
        "    4. Out = Mask * K + (1 - Mask) * I\n",
        "    Args:\n",
        "        img (Numpy array): Input image, HWC, BGR; float32, [0, 1].\n",
        "        weight (float): Sharp weight. Default: 1.\n",
        "        radius (float): Kernel size of Gaussian blur. Default: 50.\n",
        "        threshold (int):\n",
        "    \"\"\"\n",
        "    if radius % 2 == 0:\n",
        "        radius += 1\n",
        "    blur = cv2.GaussianBlur(img, (radius, radius), 0)\n",
        "    residual = img - blur\n",
        "    mask = np.abs(residual) * 255 > threshold\n",
        "    mask = mask.astype('float32')\n",
        "    soft_mask = cv2.GaussianBlur(mask, (radius, radius), 0)\n",
        "\n",
        "    K = img + weight * residual\n",
        "    K = np.clip(K, 0, 1)\n",
        "    return soft_mask * K + (1 - soft_mask) * img\n",
        "\n",
        "\n",
        "def add_blur(img, sf=4):\n",
        "    wd2 = 4.0 + sf\n",
        "    wd = 2.0 + 0.2*sf\n",
        "    if random.random() < 0.5:\n",
        "        l1 = wd2*random.random()\n",
        "        l2 = wd2*random.random()\n",
        "        k = anisotropic_Gaussian(ksize=2*random.randint(2,11)+3, theta=random.random()*np.pi, l1=l1, l2=l2)\n",
        "    else:\n",
        "        k = fspecial('gaussian', 2*random.randint(2,11)+3, wd*random.random())\n",
        "    img = ndimage.filters.convolve(img, np.expand_dims(k, axis=2), mode='mirror')\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_resize(img, sf=4):\n",
        "    rnum = np.random.rand()\n",
        "    if rnum > 0.8:  # up\n",
        "        sf1 = random.uniform(1, 2)\n",
        "    elif rnum < 0.7:  # down\n",
        "        sf1 = random.uniform(0.5/sf, 1)\n",
        "    else:\n",
        "        sf1 = 1.0\n",
        "    img = cv2.resize(img, (int(sf1*img.shape[1]), int(sf1*img.shape[0])), interpolation=random.choice([1, 2, 3]))\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_Gaussian_noise(img, noise_level1=2, noise_level2=25):\n",
        "    noise_level = random.randint(noise_level1, noise_level2)\n",
        "    rnum = np.random.rand()\n",
        "    if rnum > 0.6:   # add color Gaussian noise\n",
        "        img += np.random.normal(0, noise_level/255.0, img.shape).astype(np.float32)\n",
        "    elif rnum < 0.4: # add grayscale Gaussian noise\n",
        "        img += np.random.normal(0, noise_level/255.0, (*img.shape[:2], 1)).astype(np.float32)\n",
        "    else:            # add  noise\n",
        "        L = noise_level2/255.\n",
        "        D = np.diag(np.random.rand(3))\n",
        "        U = orth(np.random.rand(3,3))\n",
        "        conv = np.dot(np.dot(np.transpose(U), D), U)\n",
        "        img += np.random.multivariate_normal([0,0,0], np.abs(L**2*conv), img.shape[:2]).astype(np.float32)\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_speckle_noise(img, noise_level1=2, noise_level2=25):\n",
        "    noise_level = random.randint(noise_level1, noise_level2)\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    rnum = random.random()\n",
        "    if rnum > 0.6:\n",
        "        img += img*np.random.normal(0, noise_level/255.0, img.shape).astype(np.float32)\n",
        "    elif rnum < 0.4:\n",
        "        img += img*np.random.normal(0, noise_level/255.0, (*img.shape[:2], 1)).astype(np.float32)\n",
        "    else:\n",
        "        L = noise_level2/255.\n",
        "        D = np.diag(np.random.rand(3))\n",
        "        U = orth(np.random.rand(3,3))\n",
        "        conv = np.dot(np.dot(np.transpose(U), D), U)\n",
        "        img += img*np.random.multivariate_normal([0,0,0], np.abs(L**2*conv), img.shape[:2]).astype(np.float32)\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_Poisson_noise(img):\n",
        "    img = np.clip((img * 255.0).round(), 0, 255) / 255.\n",
        "    vals = 10**(2*random.random()+2.0)  # [2, 4]\n",
        "    if random.random() < 0.5:\n",
        "        img = np.random.poisson(img * vals).astype(np.float32) / vals\n",
        "    else:\n",
        "        img_gray = np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
        "        img_gray = np.clip((img_gray * 255.0).round(), 0, 255) / 255.\n",
        "        noise_gray = np.random.poisson(img_gray * vals).astype(np.float32) / vals - img_gray\n",
        "        img += noise_gray[:, :, np.newaxis]\n",
        "    img = np.clip(img, 0.0, 1.0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def add_JPEG_noise(img):\n",
        "    quality_factor = random.randint(30, 95)\n",
        "    img = cv2.cvtColor(single2uint(img), cv2.COLOR_RGB2BGR)\n",
        "    result, encimg = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), quality_factor])\n",
        "    img = cv2.imdecode(encimg, 1)\n",
        "    img = cv2.cvtColor(uint2single(img), cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "\n",
        "def random_crop(lq, hq, sf=4, lq_patchsize=64):\n",
        "    h, w = lq.shape[:2]\n",
        "    rnd_h = random.randint(0, h-lq_patchsize)\n",
        "    rnd_w = random.randint(0, w-lq_patchsize)\n",
        "    lq = lq[rnd_h:rnd_h + lq_patchsize, rnd_w:rnd_w + lq_patchsize, :]\n",
        "\n",
        "    rnd_h_H, rnd_w_H = int(rnd_h * sf), int(rnd_w * sf)\n",
        "    hq = hq[rnd_h_H:rnd_h_H + lq_patchsize*sf, rnd_w_H:rnd_w_H + lq_patchsize*sf, :]\n",
        "    return lq, hq\n",
        "\n",
        "\n",
        "def degradation_bsrgan(img, sf=4, lq_patchsize=72, isp_model=None):\n",
        "    \"\"\"\n",
        "    This is the degradation model of BSRGAN from the paper\n",
        "    \"Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\"\n",
        "    ----------\n",
        "    img: HXWXC, [0, 1], its size should be large than (lq_patchsizexsf)x(lq_patchsizexsf)\n",
        "    sf: scale factor\n",
        "    isp_model: camera ISP model\n",
        "    Returns\n",
        "    -------\n",
        "    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n",
        "    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n",
        "    \"\"\"\n",
        "    isp_prob, jpeg_prob, scale2_prob = 0.25, 0.9, 0.25\n",
        "    sf_ori = sf\n",
        "\n",
        "    h1, w1 = img.shape[:2]\n",
        "    img = img.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    if h < lq_patchsize*sf or w < lq_patchsize*sf:\n",
        "        raise ValueError(f'img size ({h1}X{w1}) is too small!')\n",
        "\n",
        "    hq = img.copy()\n",
        "\n",
        "    if sf == 4 and random.random() < scale2_prob:   # downsample1\n",
        "        if np.random.rand() < 0.5:\n",
        "            img = cv2.resize(img, (int(1/2*img.shape[1]), int(1/2*img.shape[0])), interpolation=random.choice([1,2,3]))\n",
        "        else:\n",
        "            img = imresize_np(img, 1/2, True)\n",
        "        img = np.clip(img, 0.0, 1.0)\n",
        "        sf = 2\n",
        "\n",
        "    shuffle_order = random.sample(range(7), 7)\n",
        "    idx1, idx2 = shuffle_order.index(2), shuffle_order.index(3)\n",
        "    if idx1 > idx2:  # keep downsample3 last\n",
        "        shuffle_order[idx1], shuffle_order[idx2] = shuffle_order[idx2], shuffle_order[idx1]\n",
        "\n",
        "    for i in shuffle_order:\n",
        "\n",
        "        if i == 0:\n",
        "            img = add_blur(img, sf=sf)\n",
        "\n",
        "        elif i == 1:\n",
        "            img = add_blur(img, sf=sf)\n",
        "\n",
        "        elif i == 2:\n",
        "            a, b = img.shape[1], img.shape[0]\n",
        "            # downsample2\n",
        "            if random.random() < 0.75:\n",
        "                sf1 = random.uniform(1,2*sf)\n",
        "                img = cv2.resize(img, (int(1/sf1*img.shape[1]), int(1/sf1*img.shape[0])), interpolation=random.choice([1,2,3]))\n",
        "            else:\n",
        "                k = fspecial('gaussian', 25, random.uniform(0.1, 0.6*sf))\n",
        "                k_shifted = shift_pixel(k, sf)\n",
        "                k_shifted = k_shifted/k_shifted.sum()  # blur with shifted kernel\n",
        "                img = ndimage.filters.convolve(img, np.expand_dims(k_shifted, axis=2), mode='mirror')\n",
        "                img = img[0::sf, 0::sf, ...]  # nearest downsampling\n",
        "            img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "        elif i == 3:\n",
        "            # downsample3\n",
        "            img = cv2.resize(img, (int(1/sf*a), int(1/sf*b)), interpolation=random.choice([1,2,3]))\n",
        "            img = np.clip(img, 0.0, 1.0)\n",
        "\n",
        "        elif i == 4:\n",
        "            # add Gaussian noise\n",
        "            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n",
        "\n",
        "        elif i == 5:\n",
        "            # add JPEG noise\n",
        "            if random.random() < jpeg_prob:\n",
        "                img = add_JPEG_noise(img)\n",
        "\n",
        "        elif i == 6:\n",
        "            # add processed camera sensor noise\n",
        "            if random.random() < isp_prob and isp_model is not None:\n",
        "                with torch.no_grad():\n",
        "                    img, hq = isp_model.forward(img.copy(), hq)\n",
        "\n",
        "    # add final JPEG compression noise\n",
        "    img = add_JPEG_noise(img)\n",
        "\n",
        "    # random crop\n",
        "    img, hq = random_crop(img, hq, sf_ori, lq_patchsize)\n",
        "\n",
        "    return img, hq\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def degradation_bsrgan_plus(img, sf=4, shuffle_prob=0.5, use_sharp=False, lq_patchsize=64, isp_model=None):\n",
        "    \"\"\"\n",
        "    This is an extended degradation model by combining\n",
        "    the degradation models of BSRGAN and Real-ESRGAN\n",
        "    ----------\n",
        "    img: HXWXC, [0, 1], its size should be large than (lq_patchsizexsf)x(lq_patchsizexsf)\n",
        "    sf: scale factor\n",
        "    use_shuffle: the degradation shuffle\n",
        "    use_sharp: sharpening the img\n",
        "    Returns\n",
        "    -------\n",
        "    img: low-quality patch, size: lq_patchsizeXlq_patchsizeXC, range: [0, 1]\n",
        "    hq: corresponding high-quality patch, size: (lq_patchsizexsf)X(lq_patchsizexsf)XC, range: [0, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    h1, w1 = img.shape[:2]\n",
        "    img = img.copy()[:w1 - w1 % sf, :h1 - h1 % sf, ...]  # mod crop\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    if h < lq_patchsize*sf or w < lq_patchsize*sf:\n",
        "        raise ValueError(f'img size ({h1}X{w1}) is too small!')\n",
        "\n",
        "    if use_sharp:\n",
        "        img = add_sharpening(img)\n",
        "    hq = img.copy()\n",
        "\n",
        "    if random.random() < shuffle_prob:\n",
        "        shuffle_order = random.sample(range(13), 13)\n",
        "    else:\n",
        "        shuffle_order = list(range(13))\n",
        "        # local shuffle for noise, JPEG is always the last one\n",
        "        shuffle_order[2:6] = random.sample(shuffle_order[2:6], len(range(2, 6)))\n",
        "        shuffle_order[9:13] = random.sample(shuffle_order[9:13], len(range(9, 13)))\n",
        "\n",
        "    poisson_prob, speckle_prob, isp_prob = 0.1, 0.1, 0.1\n",
        "\n",
        "    for i in shuffle_order:\n",
        "        if i == 0:\n",
        "            img = add_blur(img, sf=sf)\n",
        "        elif i == 1:\n",
        "            img = add_resize(img, sf=sf)\n",
        "        elif i == 2:\n",
        "            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n",
        "        elif i == 3:\n",
        "            if random.random() < poisson_prob:\n",
        "                img = add_Poisson_noise(img)\n",
        "        elif i == 4:\n",
        "            if random.random() < speckle_prob:\n",
        "                img = add_speckle_noise(img)\n",
        "        elif i == 5:\n",
        "            if random.random() < isp_prob and isp_model is not None:\n",
        "                with torch.no_grad():\n",
        "                    img, hq = isp_model.forward(img.copy(), hq)\n",
        "        elif i == 6:\n",
        "            img = add_JPEG_noise(img)\n",
        "        elif i == 7:\n",
        "            img = add_blur(img, sf=sf)\n",
        "        elif i == 8:\n",
        "            img = add_resize(img, sf=sf)\n",
        "        elif i == 9:\n",
        "            img = add_Gaussian_noise(img, noise_level1=2, noise_level2=25)\n",
        "        elif i == 10:\n",
        "            if random.random() < poisson_prob:\n",
        "                img = add_Poisson_noise(img)\n",
        "        elif i == 11:\n",
        "            if random.random() < speckle_prob:\n",
        "                img = add_speckle_noise(img)\n",
        "        elif i == 12:\n",
        "            if random.random() < isp_prob and isp_model is not None:\n",
        "                with torch.no_grad():\n",
        "                    img, hq = isp_model.forward(img.copy(), hq)\n",
        "        else:\n",
        "            print('check the shuffle!')\n",
        "\n",
        "    # resize to desired size\n",
        "    img = cv2.resize(img, (int(1/sf*hq.shape[1]), int(1/sf*hq.shape[0])), interpolation=random.choice([1, 2, 3]))\n",
        "\n",
        "    # add final JPEG compression noise\n",
        "    img = add_JPEG_noise(img)\n",
        "\n",
        "    # random crop\n",
        "    img, hq = random_crop(img, hq, sf, lq_patchsize)\n",
        "\n",
        "    return img, hq"
      ],
      "metadata": {
        "id": "zKh_Oj4QtZgM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model with SR network \n",
        "model = net(upscale=4, in_chans=3, img_size=64, window_size=8,\n",
        "                        img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
        "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='1conv')\n",
        "\n",
        "pretrained_model = torch.load(r'../pretrained_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth')\n",
        "model.load_state_dict(pretrained_model['params_ema'], strict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFULY8mtJ___",
        "outputId": "18ca850e-e2a8-4d39-f8cf-4ee4411aa152"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "import argparse\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "folder = \"../Desktop/train/\" \n",
        "save_dir = \"../Desktop/result/\"\n",
        "border = 0 \n",
        "window_size = 8\n",
        "scale = 4\n",
        "test_results = OrderedDict()\n",
        "test_results['psnr'] = []\n",
        "test_results['ssim'] = []\n",
        "test_results['psnr_y'] = []\n",
        "test_results['ssim_y'] = []\n",
        "test_results['psnr_b'] = []\n",
        "psnr, ssim, psnr_y, ssim_y, psnr_b = 0, 0, 0, 0, 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6o_idVTDB8j",
        "outputId": "430b5ac3-fba8-4cf7-bb6a-a7eb56fd4992"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to return low resolution and high resolution image. Function generates low resolution image using BSRGAN. \n",
        "def get_image_pair(path, degrade=True):\n",
        "    (imgname, imgext) = os.path.splitext(os.path.basename(path))\n",
        "    if degrade:\n",
        "      img_gt = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
        "      img_L, img_H = degradation_bsrgan(img_gt)\n",
        "      return imgname, img_L, img_H\n",
        "    else:\n",
        "      img_L = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
        "      img_H = None\n",
        "      return imgname, img_L, img_H\n",
        "\n",
        "\n",
        "def get_img_tensor(img_lq):\n",
        "  img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "  img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "  return img_lq"
      ],
      "metadata": {
        "id": "QfZ2VU4zGsdH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perceptual loss implementation\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "# --------------------------------------------\n",
        "# Perceptual loss\n",
        "# --------------------------------------------\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_layer=[2,7,16,25,34], use_input_norm=True, use_range_norm=False):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "        '''\n",
        "        use_input_norm: If True, x: [0, 1] --> (x - mean) / std\n",
        "        use_range_norm: If True, x: [0, 1] --> x: [-1, 1]\n",
        "        '''\n",
        "        model = torchvision.models.vgg19(pretrained=True)\n",
        "        self.use_input_norm = use_input_norm\n",
        "        self.use_range_norm = use_range_norm\n",
        "        if self.use_input_norm:\n",
        "            mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "            std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "        self.list_outputs = isinstance(feature_layer, list)\n",
        "        if self.list_outputs:\n",
        "            self.features = nn.Sequential()\n",
        "            feature_layer = [-1] + feature_layer\n",
        "            for i in range(len(feature_layer)-1):\n",
        "                self.features.add_module('child'+str(i), nn.Sequential(*list(model.features.children())[(feature_layer[i]+1):(feature_layer[i+1]+1)]))\n",
        "        else:\n",
        "            self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n",
        "\n",
        "        print(self.features)\n",
        "\n",
        "        # No need to BP to variable\n",
        "        for k, v in self.features.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_range_norm:\n",
        "            x = (x + 1.0) / 2.0\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "        if self.list_outputs:\n",
        "            output = []\n",
        "            for child_model in self.features.children():\n",
        "                x = child_model(x)\n",
        "                output.append(x.clone())\n",
        "            return output\n",
        "        else:\n",
        "            return self.features(x)\n",
        "\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    \"\"\"VGG Perceptual loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_layer=[2,7,16,25,34], weights=[0.1,0.1,1.0,1.0,1.0], lossfn_type='l1', use_input_norm=True, use_range_norm=False):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.vgg = VGGFeatureExtractor(feature_layer=feature_layer, use_input_norm=use_input_norm, use_range_norm=use_range_norm)\n",
        "        self.lossfn_type = lossfn_type\n",
        "        self.weights = weights\n",
        "        if self.lossfn_type == 'l1':\n",
        "            self.lossfn = nn.L1Loss()\n",
        "        else:\n",
        "            self.lossfn = nn.MSELoss()\n",
        "        print(f'feature_layer: {feature_layer}  with weights: {weights}')\n",
        "\n",
        "    def forward(self, x, gt):\n",
        "        \"\"\"Forward function.\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
        "            gt (Tensor): Ground-truth tensor with shape (n, c, h, w).\n",
        "        Returns:\n",
        "            Tensor: Forward results.\n",
        "        \"\"\"\n",
        "        x_vgg, gt_vgg = self.vgg(x), self.vgg(gt.detach())\n",
        "        loss = 0.0\n",
        "        if isinstance(x_vgg, list):\n",
        "            n = len(x_vgg)\n",
        "            for i in range(n):\n",
        "                loss += self.weights[i] * self.lossfn(x_vgg[i], gt_vgg[i])\n",
        "        else:\n",
        "            loss += self.lossfn(x_vgg, gt_vgg.detach())\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "qxzPzGiuYai9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function setup, which is a combination of L1 and Perceptual loss. Optimizer setup using ADAM.\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "\n",
        "def fft_map(x):\n",
        "    fft_x = torch.fft.fftn(x)\n",
        "    fft_x_real = fft_x.real\n",
        "    fft_x_imag = fft_x.imag\n",
        "\n",
        "    return fft_x_real, fft_x_imag \n",
        "# Loss\n",
        "G_lossfn = nn.L1Loss().to(device)\n",
        "perceptual_lossfn = PerceptualLoss().to(device)\n",
        "G_lossfn_weight = 1\n",
        "\n",
        "def total_loss(E, H):\n",
        "  alpha = 15\n",
        "  beta = 0.1\n",
        "  gamma = 0.0025\n",
        "\n",
        "  # H HR, E Recon, L LR\n",
        "  H_k_real, H_k_imag = fft_map(H)\n",
        "  E_k_real, E_k_imag = fft_map(E)\n",
        "\n",
        "  loss_image = G_lossfn(E, H)\n",
        "  loss_freq = (G_lossfn(E_k_real, H_k_real) + G_lossfn(E_k_imag, H_k_imag)) / 2\n",
        "  loss_perc = perceptual_lossfn(E, H)\n",
        "\n",
        "  return alpha * loss_image + beta * loss_freq + gamma * loss_perc\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 2e-4\n",
        "G_optim_params = []\n",
        "for k, v in model.named_parameters():\n",
        "  if v.requires_grad:\n",
        "    G_optim_params.append(v)\n",
        "  else:\n",
        "    print('Params [{:s}] will not optimize.'.format(k))\n",
        "G_optimizer = Adam(G_optim_params, learning_rate, weight_decay=0)"
      ],
      "metadata": {
        "id": "EzpSzX1fHRca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8b8d0c-306a-416d-cb0b-85a48acc401c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (child0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child1): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child2): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child3): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (child4): Sequential(\n",
            "    (0): ReLU(inplace=True)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "feature_layer: [2, 7, 16, 25, 34]  with weights: [0.1, 0.1, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Validation\n",
        "for epoch in range(5):\n",
        "  #train\n",
        "  model.train()\n",
        "  for i, path in enumerate(sorted(glob.glob(os.path.join(folder, '*')))):\n",
        "    # read image\n",
        "    imgname, img_lq, img_gt = get_image_pair(path)  # image to HWC-BGR, float32\n",
        "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "    img_gt = get_img_tensor(img_gt)\n",
        "\n",
        "    # pad input image to be a multiple of window_size\n",
        "    _, _, h_old, w_old = img_lq.size()\n",
        "    h_pad = (h_old // window_size + 1) * window_size - h_old\n",
        "    w_pad = (w_old // window_size + 1) * window_size - w_old\n",
        "    img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "    img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "    output = model(img_lq)\n",
        "    output = output[..., :h_old * scale, :w_old * scale]\n",
        "    loss = total_loss(output, img_gt)\n",
        "    G_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "  #validation\n",
        "  model.eval()\n",
        "  idx = 0\n",
        "  avg_psnr = 0.0\n",
        "  for idx, path in enumerate(sorted(glob.glob(os.path.join('../Desktop/validation/', '*')))):\n",
        "    idx += 1\n",
        "    # read image\n",
        "    imgname, img_lq, img_gt = get_image_pair(path, True)  # image to HWC-BGR, float32\n",
        "    img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "    img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # pad input image to be a multiple of window_size\n",
        "      _, _, h_old, w_old = img_lq.size()\n",
        "      h_pad = (h_old // window_size + 1) * window_size - h_old\n",
        "      w_pad = (w_old // window_size + 1) * window_size - w_old\n",
        "      img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "      img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "      output = model(img_lq)\n",
        "      output = output[..., :h_old * scale, :w_old * scale]\n",
        "\n",
        "      img_gt = get_img_tensor(img_gt)\n",
        "      E_img = tensor2uint(output)\n",
        "      H_img = tensor2uint(img_gt)\n",
        "\n",
        "    # -----------------------\n",
        "    # calculate PSNR\n",
        "    # -----------------------\n",
        "    current_psnr = calculate_psnr(E_img, H_img, border=border)\n",
        "    avg_psnr += current_psnr\n",
        "\n",
        "  avg_psnr = avg_psnr / idx\n",
        "  print(\"Average PSNR: \", avg_psnr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfTffKEPXEBF",
        "outputId": "0c0383dc-b4f9-49ae-a3f4-4d87c1faa1d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/folders/ly/xbr0bb4d12s4f_jr2r2w1xrr0000gn/T/ipykernel_11318/3273088679.py:331: DeprecationWarning: Please use `convolve` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  img = ndimage.filters.convolve(img, np.expand_dims(k, axis=2), mode='mirror')\n",
            "/var/folders/ly/xbr0bb4d12s4f_jr2r2w1xrr0000gn/T/ipykernel_11318/3273088679.py:475: DeprecationWarning: Please use `convolve` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  img = ndimage.filters.convolve(img, np.expand_dims(k_shifted, axis=2), mode='mirror')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average PSNR:  24.309989679055892\n",
            "Average PSNR:  26.74641404859804\n",
            "Average PSNR:  26.69878608011634\n",
            "Average PSNR:  27.317388404477526\n",
            "Average PSNR:  26.18571235406673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model on desktop. make sure to create new_model folder on desktop.\n",
        "torch.save(model.state_dict(), '../Desktop/new_model/model_weights.pth')"
      ],
      "metadata": {
        "id": "wcR8-Tol_xZJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model \n",
        "# Step 1: Make sure model model_weights.pth exists in Desktop/new_model/ \n",
        "# Step 2: Save the images you want to test on in Desktop/test/\n",
        "# Step 3: Create desktop/result folder\n",
        "# Step 3: Run this step\n",
        "# Step 5: Check result in result folder\n",
        "\n",
        "save_dir = \"../Desktop/result/\"\n",
        "model = net(upscale=4, in_chans=3, img_size=64, window_size=8,\n",
        "                        img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
        "                        mlp_ratio=2, upsampler='nearest+conv', resi_connection='1conv')\n",
        "\n",
        "pretrained_model = torch.load(r'../Desktop/new_model/model_weights.pth')\n",
        "#model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.load_state_dict(pretrained_model)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "for idx, path in enumerate(sorted(glob.glob(os.path.join('../Desktop/test/', '*')))):\n",
        "  # read image\n",
        "  imgname, img_lq, img_gt = get_image_pair(path, False)  # image to HWC-BGR, float32\n",
        "  img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1))  # HCW-BGR to CHW-RGB\n",
        "  img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device)  # CHW-RGB to NCHW-RGB\n",
        "\n",
        "  # inference\n",
        "  with torch.no_grad():\n",
        "    # pad input image to be a multiple of window_size\n",
        "    _, _, h_old, w_old = img_lq.size()\n",
        "    h_pad = (h_old // window_size + 1) * window_size - h_old\n",
        "    w_pad = (w_old // window_size + 1) * window_size - w_old\n",
        "    img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
        "    img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
        "    output = model(img_lq)\n",
        "    output = output[..., :h_old * scale, :w_old * scale]\n",
        "\n",
        "    # save image\n",
        "    output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    if output.ndim == 3:\n",
        "       output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # CHW-RGB to HCW-BGR\n",
        "    output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
        "    cv2.imwrite(f'{save_dir}/{imgname}_SwinIR.png', output)"
      ],
      "metadata": {
        "id": "oHLf32xuF3qc"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}